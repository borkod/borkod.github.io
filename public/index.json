[{"categories":null,"content":"Recent updates related to Kubernetes","date":"2020-08-19","objectID":"/musings-on-kubernetes/","tags":["Azure","Microsoft","Kubernetes","AKS","Azure Kubernetes Service"],"title":"Musings on Kubernetes","uri":"/musings-on-kubernetes/"},{"categories":null,"content":"It has been really busy few months! Over the past few months, my work has started focusing very heavily on Kubernetes technology. Coupled with the global COVID situation eliminating summer travel and social activities, I‚Äôve taken that opportunity to really immerse myself into the Kubernetes technology ecosystem. I thought I‚Äôd share a few random thoughts/ideas/opinions I‚Äôve developed as I‚Äôve been expanding my knowledge and experience with this technology. ","date":"2020-08-19","objectID":"/musings-on-kubernetes/:0:0","tags":["Azure","Microsoft","Kubernetes","AKS","Azure Kubernetes Service"],"title":"Musings on Kubernetes","uri":"/musings-on-kubernetes/"},{"categories":null,"content":"Certifications Cloud Native Computing Foundation (CNCF), in collaboration with The Linux Foundation, offers two certification programs for those looking to validate their Kubernetes knowledge or obtain an industry accreditation. The Certified Kubernetes Administrator (CKA) program focuses on Kubernetes cluster administration topics with special attention given to cluster installation, configuration and maintenance, networking, security, and troubleshooting. The Certified Kubernetes Application Developer (CKAD) exam places focus on defining Kubernetes resources and primitives related to deploying applications on top of the platform. I ended up writing both exams and was successful in obtaining both certifications. In contrast to many certification exams, these tests are all hands-on exercises with no academic or multiple choice questions. Both exams are challenging in their own respect. Common to both tests is that you‚Äôre really time constrained. My experience was that you had to work through the questions very quickly in order to complete the exam and do well. CKA exam is longer, covers more content and wider range of topics, and requires a deeper knowledge and understanding of networking and Linux OS administration. CKAD is easier in a sense that it is shorter and covers less topics. However, it provides less time on per question basis, making it also a challenging test. CNCF recently announced addition of a new Certified Kubernetes Security Specialist (CKS) program. I will definitely be looking to write that exam once the certification becomes generally available in November. ","date":"2020-08-19","objectID":"/musings-on-kubernetes/:1:0","tags":["Azure","Microsoft","Kubernetes","AKS","Azure Kubernetes Service"],"title":"Musings on Kubernetes","uri":"/musings-on-kubernetes/"},{"categories":null,"content":"Training Resources There are a plethora of resources out there online for tips and guides on preparing for these exams. I will just high-light a couple. For online training courses, The Linux Foundation offers training courses for both CKA and CKAD. Udemy also has a couple of good courses for those interested in preparing for the exams (CKA and CKAD). All of these courses provide hands-on exercises for gaining practical familiarity working with Kubernetes. From my experience, I would recommend Udemy courses slightly over the Linux Foundation courses, especially taking into account the price and value. For exercises, there are a lot of resource to be found online. Kubernetes The Hard Way is the canonical go-to resource for anybody looking to delve deep into cluster administration (and preparing for the CKA exam). I found this GitHub repo to contain a pretty complete list of review exercises for CKAD exam. The best and final tip I would offer is to just stand up a Kubernetes cluster and start developing. Practice! Practice! Practice! ","date":"2020-08-19","objectID":"/musings-on-kubernetes/:2:0","tags":["Azure","Microsoft","Kubernetes","AKS","Azure Kubernetes Service"],"title":"Musings on Kubernetes","uri":"/musings-on-kubernetes/"},{"categories":null,"content":"Getting Started Installing the kubectl command line tool is the first step for working with Kubernetes. Azure and AWS cloud providers offer managed Kubernetes services ideal for getting started quickly. To get started on Azure: # Create a resource group az group create --name \u003cresource group\u003e --location \u003clocation\u003e # Create Azure Kubernetes Cluster az aks create --resource-group \u003cresource group\u003e --name \u003cAKS Name\u003e --node-count 3 # Get Login Credentials # az aks get-credentials --resource-group \u003cresource group\u003e --name \u003cAKS Name\u003e To get started on AWS: eksctl create cluster --name \u003cEKS Name\u003e --version 1.16 --region \u003cAWS region\u003e --nodegroup-name \u003cnode group name\u003e --node-type t3.medium --nodes 3 --nodes-min 1 --nodes-max 4 --managed For a local installation, Minikube offers an easy and quick way to get started with a learning environment. ","date":"2020-08-19","objectID":"/musings-on-kubernetes/:3:0","tags":["Azure","Microsoft","Kubernetes","AKS","Azure Kubernetes Service"],"title":"Musings on Kubernetes","uri":"/musings-on-kubernetes/"},{"categories":null,"content":"Comparison of Azure Traffic Manager and Azure Front Door","date":"2020-03-29","objectID":"/azure-global-load-balancing-comparison/","tags":["Azure","Microsoft","Load Balancing","Traffic Manager","Front Door"],"title":"Azure Global Load Balancing Comparison","uri":"/azure-global-load-balancing-comparison/"},{"categories":null,"content":"Global load-balancing services distribute traffic across applications and endpoints hosted across different regions/geographies. These services route end-user traffic to the available backend based on configurable routing rules. They also feature endpoint monitoring in order to improve high availability and reliability of backend services. Azure offers two global load-balancing services: Azure Traffic Manager and Azure Front Door. Traffic Manager is a DNS-based traffic load balancer that distributes traffic to services across global Azure regions. Being a DNS-based load-balancing service, it load balances only at the domain level. As such, it supports any communication protocol and has no performance impact. Front Door is global load balancing service specifically designed for web applications. Therefore, it works at Layer 7 and offers many capabilities useful for web-based application (e.g SSL offloading, path-based routing, fast fail-over, caching, WAF, etc.) I thought it would be useful to compile a list of features offered by the two services so that they can be easily and quickly compared and contrasted. Azure documentation offers a very practical decision tree that‚Äôs great for determining the right load-balancing solution for various scenarios. I hope the table below is helpful! Feature Traffic Manager Front Door Protocol Protocol Agnostic (DNS based) HTTP(S) Protocol Support: HTTP/2 ‚úîÔ∏è ‚úîÔ∏è Protocol Suppport: IPv6 ‚úîÔ∏è ‚úîÔ∏è Supported Backends1 Any public IP or a publicly resolvable DNS hostname Nested endpoints (used to combine Traffic Manager profiles) Any public IP or a publicly resolvable DNS hostname Non-Azure Backends ‚úîÔ∏è ‚úîÔ∏è Backend must be public ‚úîÔ∏è ‚úîÔ∏è Nesting ‚úîÔ∏è ‚ùå Endpoint Monitoring ‚úîÔ∏è ‚úîÔ∏è Fast Fail-over ‚ùå2 ‚úîÔ∏è Routing Methods Performance routing Priority routing Weighted round-robin routing Geographic routing Multivalue Subnet routing Latency Routing Priority Routing Weighted Routing Geographic Routing (vai WAF policy) HTTP Property Routing ‚ùå ‚úîÔ∏è Session Affinity ‚ùå ‚úîÔ∏è TLS Termination ‚ùå ‚úîÔ∏è Custom Domain ‚úîÔ∏è3 ‚úîÔ∏è Wildcard domains ‚úîÔ∏è ‚úîÔ∏è Certificate Management ‚ùå ‚úîÔ∏è self-signed certificates ‚úîÔ∏è ‚ùå HTTPS Redirection ‚ùå ‚úîÔ∏è Re-encryption ‚ùå4 ‚úîÔ∏è5 WAF ‚ùå ‚úîÔ∏è DDoS ‚ùå ‚úîÔ∏è Caching ‚ùå ‚úîÔ∏è Content Compression ‚ùå ‚úîÔ∏è Session Affinity ‚ùå ‚úîÔ∏è URL Rewrite ‚ùå ‚úîÔ∏è URL redirect ‚ùå ‚úîÔ∏è Traffic View Dashboard ‚úîÔ∏è6 ‚ùå No Performance Impact ‚úîÔ∏è7 ‚ùå Pricing Considerations DNS queries received Monitored endpoints User Measurements Traffic View Data points Outbound data transfer Inbound data transfer Routing Rules Custom Domains WAF Service Fee WAF Rulesets WAF Custom Rules Azure Endpoints (PaaS Cloud Services, Storage, Web Apps) Public IP Address Resources (e.g. VM‚Äôs with public IPs or Azure Load Balancer) External Backends (Public IPv4/IPv6 addresses, FQDNs) ‚Ü©Ô∏é Traffic Manager fail over limited by DNS caching/TTL ‚Ü©Ô∏é Traffic Manager supports Custom Domain via CNAME or Alias Records. ‚Ü©Ô∏é Traffic Manager doesn‚Äôt support TLS Termination, so re-encryption isn‚Äôt necessary. ‚Ü©Ô∏é Front Door connections to the backend happen over public IP, so it is recommended that Front Door is configured to use HTTPS as the forwarding protocol. ‚Ü©Ô∏é Traffic Manager Traffic View provides a dashboard of user base regions and their traffic patterns. ‚Ü©Ô∏é Traffic Manager works at the DNS level. Client connections are direct to service endpoints, and so there is no performance impact incurred when using Traffic Manager. ‚Ü©Ô∏é ","date":"2020-03-29","objectID":"/azure-global-load-balancing-comparison/:0:0","tags":["Azure","Microsoft","Load Balancing","Traffic Manager","Front Door"],"title":"Azure Global Load Balancing Comparison","uri":"/azure-global-load-balancing-comparison/"},{"categories":null,"content":"Azure Functions Binding Expression Parameterization","date":"2020-03-09","objectID":"/azure-functions-binding-expression-parameterization/","tags":["Azure","Microsoft","Functions","Serverless","Tips and Tricks"],"title":"Azure Functions Binding Expression Parameterization","uri":"/azure-functions-binding-expression-parameterization/"},{"categories":null,"content":"Azure Functions is an event driven, serverless computing service provided by Microsoft Azure Cloud platform. Azure Functions allows developers to deploy code connecting to data sources or messaging solutions thus enabling easier implementation of event driven processing of messages. Azure Functions, like rest of serverless computing, is designed to accelerate and simplify application development by removing the responsibility of infrastructure management. Azure Functions uses concept of bindings as a way of declaratively connecting another resource to the function. Bindings may be connected as input bindings, output bindings, or both, and can be used as triggers for message and event processing. Few commonly used Azure Function bindings are: Blog Storage Cosmos DB Event Grid Event Hubs If implementing in JavaScript, bindings are specified in function.json as shown in below example: { \"type\": \"eventHubTrigger\", \"name\": \"eventHubMessages\", \"direction\": \"in\", \"eventHubName\": \"defaulthub\", \"connection\": \"eventsamples_defaultReadPolicy_EVENTHUB\" } Here, eventsamples_defaultReadPolicy_EVENTHUB refers to the Azure Function Application Setting that specifies the Event Hub Connection strings. Similar binding definitions are available for other supported services. It is possible to specify few other parameters in the binding definition. For example, the above trigger binding for Event Hubs can include additional parameters for cardinality or the consumer group: { ... \"cardinality\": \"many\", \"consumerGroup\": \"default\" } One issue arises if we wish to re-use the same function with only a single change that is the target consumer group. Ideally we would want to set the consumer group in an environmental variable in the application settings. How does one load in a consumer group from an Application setting? Similar problem can occur with other binding sources. For example, path setting for Blob storage, databaseName or collectionName for Cosmos DB. A nice trick I recently found was to use binding expressions. In your function.json file, you can specify consumer group name inside % symbol: { ... \"consumerGroup\": \"%customGroupName%\" ... } Then add an Application setting with corresponding name (customGroupName in this case). Here are couple of screenshots of the function settings and application settings with these updates. Consumer group will then be resolved to the value in Application Settings at startup. Same method can be used for binding for other services and it works for trigger, input, and output bindings. Hope this is helpful! ","date":"2020-03-09","objectID":"/azure-functions-binding-expression-parameterization/:0:0","tags":["Azure","Microsoft","Functions","Serverless","Tips and Tricks"],"title":"Azure Functions Binding Expression Parameterization","uri":"/azure-functions-binding-expression-parameterization/"},{"categories":null,"content":"Summary review of Microsoft Ignite the Tour Conference in Toronto in January 2020","date":"2020-01-11","objectID":"/microsoft-ignite-the-tour-toronto-recap/","tags":["Azure","Microsoft","Microsoft Ignite the Tour","Toronto","Recap"],"title":"Microsoft Ignite the Tour Toronto Recap","uri":"/microsoft-ignite-the-tour-toronto-recap/"},{"categories":null,"content":"Microsoft Ignite the Tour conference is a two-day version of Microsoft‚Äôs annual Ignite technology conference that Microsoft is bringing to a number of cities around the world. Last week I had the opportunity to attend the conference in Toronto. As usual at these types of events, there was ‚ÄúThe Hub‚Äù, a central meeting place where large number of sponsors had booths set up demonstrating their products or services and giving out SWAG. The Hub also contained three theaters that featured 15-minute lightning sessions on technical topics from Microsoft experts or customer discussion panels. The conference agenda was also packed with deep-dive breakout sessions on large variety of topics. ‚ö°Ô∏è ‚ÄúHighlights from #MSIgniteTheTour Toronto Day 1‚Äùhttps://t.co/oPvCLNWS1a ‚Äî Microsoft IT Pro Canada (@MS_ITProCA) January 8, 2020 ","date":"2020-01-11","objectID":"/microsoft-ignite-the-tour-toronto-recap/:0:0","tags":["Azure","Microsoft","Microsoft Ignite the Tour","Toronto","Recap"],"title":"Microsoft Ignite the Tour Toronto Recap","uri":"/microsoft-ignite-the-tour-toronto-recap/"},{"categories":null,"content":"Learning Paths A unique and differentiating aspect of this conference was that the breakout sessions consisted of Learning Paths. These were essentially educational sessions connected around a common theme. Learning Paths focused on several themes, including Dynamics 365, Microsoft Teams, Windows, AI, Azure, among others. I attended sessions of the two Learning Path tracks that seemed most pertinent to me, Developing cloud native applications and Modernizing web applications and data. Each Learning Path breakout session aimed its attention at a different technical aspect, e.g. migrating data, compute or data storage services, operations, etc. Another interesting and unique element of the learning paths is that they were all linked via a common story dealing with a fictitious company ‚ÄúTailwind Traders‚Äù. Every individual session presented a scenario that was used to illustrate some technical capabilities and services provided by Azure. Tailwind Traders is a collection of Azure reference apps used to showcase Azure technology. Although this scenario-based format is very effective as a teaching method, I think I would have preferred if breakout sessions involved real-life customer stories instead. The sessions were capped at 45 minutes and there was no time for any Q and A during the sessions. Instead, the presenters were available for Q and A at the Microsoft Showcase booth in The Hub. I didn‚Äôt particularly like this. There is a lot of learning value in hearing questions or issues brought up by others in the audience and having answers shared with the entire group. All breakout sessions used speech to text to provide real-time subtitles of the presenter lecture. Kudos go to Microsoft for improving accessibility to the conference sessions in this way. üëç For myself, the highlights of the breakout sessions were the two presentations given by Aaron Wislang (@as_w). The first session was APPS30: Modernizing Your Application with Containers. This was followed by a second session on APPS40: Consolidating Infrastructure with Azure Kubernetes Service. Both presentations were very well prepared and adeptly communicated to the audience. The content of the talks presented a great review of fundamentals of container technology and an introduction to the related services in the Azure cloud platform. The demos were also effective in illustrating the concepts and services discussed in the presentation. I also recommend visiting Aaron‚Äôs compilation of useful Kubernetes resources. Another notable session I attended was by April Edwards (@theapriledwards) titled [MOD50 Managing Delivery of your App via DevOps](https://github.com/microsoft/ignite-learning-paths-training-mod/tree/master/mod50). The talk was very well polished and presented. It provided the audience a very good introductory overview DevOps and CI/CD. I liked that there was a focus on people and culture as much as technology. For the technical aspects, the session presented a good introduction to Azure DevOps for those not familiar with the platform. It included demonstrations of Azure DevOps integration with Github Actions, canary deployments utilizing deployment slots, and approval gates. ","date":"2020-01-11","objectID":"/microsoft-ignite-the-tour-toronto-recap/:1:0","tags":["Azure","Microsoft","Microsoft Ignite the Tour","Toronto","Recap"],"title":"Microsoft Ignite the Tour Toronto Recap","uri":"/microsoft-ignite-the-tour-toronto-recap/"},{"categories":null,"content":"Theater Talks One of the theater talks with special interest to me was the panel discussion with the City of Ottawa on transformation and upskilling. The sessions turned out to be especially popular as there was a very large audience gathered around the theater area. \"As we embark on this journey of transformation, the use of technology has changed from what it used to be.\" - Fawad Ahmed, City of Ottawa @ottawacity #MSIgniteTheTour pic.twitter.com/ukvDSV0ueH ‚Äî Microsoft Canada (@microsoftcanada) January 8, 2020 The panel conversation focused on City of Ottawa‚Äôs digital transformation journey. The speakers shared insights around challenges and lessons learned as they adopted Dynamics 365 and Azure platforms to transform city‚Äôs public services. ","date":"2020-01-11","objectID":"/microsoft-ignite-the-tour-toronto-recap/:2:0","tags":["Azure","Microsoft","Microsoft Ignite the Tour","Toronto","Recap"],"title":"Microsoft Ignite the Tour Toronto Recap","uri":"/microsoft-ignite-the-tour-toronto-recap/"},{"categories":null,"content":"Announcements During the conference, Microsoft made a news release announcing addition of Availability Zones in the Canada Central region and increased compute capacity in both Canadian regions. This is great news for Canada and will positively impact cloud adoption here. Today, at #MSEnvisionTheTour, we announced that we are making significant investments in our üá®üá¶ cloud with the addition of #Azure Availability Zones, and new Azure ExpressRoute in Vancouver. Read more about our news here: https://t.co/yMUuq7KlZP pic.twitter.com/3Bkv2hrsG5 ‚Äî Microsoft Canada (@microsoftcanada) January 9, 2020 ","date":"2020-01-11","objectID":"/microsoft-ignite-the-tour-toronto-recap/:3:0","tags":["Azure","Microsoft","Microsoft Ignite the Tour","Toronto","Recap"],"title":"Microsoft Ignite the Tour Toronto Recap","uri":"/microsoft-ignite-the-tour-toronto-recap/"},{"categories":null,"content":"Free Certifications Microsoft Learn section of the Microsoft Showcase booth was always buzzing with people interested in finding out more about trainings and certifications offered by Microsoft. All conference attendees were able to receive an offer for a free certification exam voucher. I am definitely eager to take advantage of this on one of my future tests! ","date":"2020-01-11","objectID":"/microsoft-ignite-the-tour-toronto-recap/:4:0","tags":["Azure","Microsoft","Microsoft Ignite the Tour","Toronto","Recap"],"title":"Microsoft Ignite the Tour Toronto Recap","uri":"/microsoft-ignite-the-tour-toronto-recap/"},{"categories":null,"content":"Microsoft Reactor - Toronto Also in the Microsoft Showcase area, I was very excited to learn about Microsoft Reactor initiative and its newly opened Toronto location. This seems like a tremendous program and an invaluable resource. I‚Äôd love to see it expanded to more cities in Canada (especially Ottawa! üòä). ","date":"2020-01-11","objectID":"/microsoft-ignite-the-tour-toronto-recap/:5:0","tags":["Azure","Microsoft","Microsoft Ignite the Tour","Toronto","Recap"],"title":"Microsoft Ignite the Tour Toronto Recap","uri":"/microsoft-ignite-the-tour-toronto-recap/"},{"categories":null,"content":"Final Thoughts This was my first Microsoft Ignite experience. Although it was two very busy days, the conference was a very positive experience. It was a great way to kick off 2020 and I am excited to see where Azure goes over the next twelve months! ","date":"2020-01-11","objectID":"/microsoft-ignite-the-tour-toronto-recap/:6:0","tags":["Azure","Microsoft","Microsoft Ignite the Tour","Toronto","Recap"],"title":"Microsoft Ignite the Tour Toronto Recap","uri":"/microsoft-ignite-the-tour-toronto-recap/"},{"categories":null,"content":"AWS Certified Database - Specialty Exam Preparation Guide and Study Tips","date":"2020-01-03","objectID":"/aws-certified-database-specialty-exam-guide/","tags":["AWS","Certification","Database","Study Guide","Tips and Tricks"],"title":"AWS Certified Database - Specialty Exam Preparation Guide and Study Tips","uri":"/aws-certified-database-specialty-exam-guide/"},{"categories":null,"content":"In late November of 2019, AWS announced a new specialty certification focusing specifically on database technologies. The AWS Certified Database - Specialty Exam beta period started in early December of 2019 with the standard certification exam availability target date being April of 2020. At the start of December, I participated in the beta attempting the beta exams. As of this writing, I do not know my results and whether on not I achieved the certification. Nevertheless, I thought it would be useful to share my experience. I‚Äôll try to list the topics covered by the exam questions, and provide some insight and resources that are helpful in preparing for the exam. The contents of this blog post can also be found in my GitHub repo here. ","date":"2020-01-03","objectID":"/aws-certified-database-specialty-exam-guide/:0:0","tags":["AWS","Certification","Database","Study Guide","Tips and Tricks"],"title":"AWS Certified Database - Specialty Exam Preparation Guide and Study Tips","uri":"/aws-certified-database-specialty-exam-guide/"},{"categories":null,"content":"Exam Description The database certification exam covers five domains: Domain 1: Workload-Specific Database Design (26%) Domain 2: Deployment and Migration (20%) Domain 3: Management and Operations (18%) Domain 4: Monitoring and Troubleshooting (18%) Domain 5: Database Security (18%) The exam‚Äôs goal is to test user‚Äôs competence to: Understand the various AWS database services Recommend and design database solutions appropriate to satisfy specific problem requirements AWS suggests that the examinee has: Minimum of 5 years of experience working with relational and NoSQL databases, including on-prem and cloud based implementations Minimum of 2 years of hand-on experience with AWS ","date":"2020-01-03","objectID":"/aws-certified-database-specialty-exam-guide/:1:0","tags":["AWS","Certification","Database","Study Guide","Tips and Tricks"],"title":"AWS Certified Database - Specialty Exam Preparation Guide and Study Tips","uri":"/aws-certified-database-specialty-exam-guide/"},{"categories":null,"content":"Exam Topics Amazon RDS is one of the core services offered by AWS, so having good understanding and thorough knowledge of this service is critical to succeeding in the exam. You should have expertise in analyzing and identifying requirements / use cases when a relational database (and specifically RDS) is the appropriate solution. You should also have a good understanding of the service capabilities and limitations. Multi-AZ: Know technical details of its implementation and when it is appropriate (e.g. disaster recovery scenarios) Read-Replicas: Know technical details and in what situations is it useful (e.g. off-loading read traffic) Know the numerous engine options RDS has available There are engine specific questions, so you should have some familiarity with each engine (e.g. Oracle TDE, PostgreSQL specific capabilities) Backups Option Groups DB Parameter Groups SQL Performance Insights (what is it, when is it applicable, and how is it different from CloudWatch) Alongside RDS, DynamoDB is probably the most crucial of AWS database services. You should know it inside and out. Understand Primary Keys/Partition Keys/Sort Keys. Given a particular scenario, be able to design/choose them. Global Secondary Indexes / Local Secondary Indexes. Understand difference between them and applicable use cases. DynamoDB Streams Global Tables (and its use cases) Data Modeling Partition Sharding (e.g. adding random suffix) Composite Keys Design patterns and best practices Querying and Filtering DAX (vs ElastiCache) Have a good understanding of the underlying technical architecture Amazon Aurora Understand the technical architecture and use cases Difference between Aurora Read Replicas/RDS Read Replicas Scaling of Aurora Understand Aurora Serverless Understand Aurora Clones and when to you should use them CloudWatch Understand the service functionality How to use in troubleshooting scenarios Performance monitoring and notification/alerting scenarios Database Migration There are questions around AWS SCT (Schema Conversion Tool) and AWS DMS (Database Migration Service). You should have a general understanding of both, difference between them, and when you would use one vs the other. ElastiCache Have a good understanding of use cases when caching is appropriate Focus on Redis Understand Redis architecture, Multi-AZ, etc. Understand various caching strategies (Lazy Loading, Write-Through) and benefits/limitation of each DocumentDB I don‚Äôt recall many questions on this service, however you should have general familiarity with it Redshift Have a general understanding of data-warehouse topics and columnar storage Be familiar with technical architecture (single node deployment / cluster with leader and compute nodes) Be familiar with Redshift Spectrum Understand use cases for Redshift (e.g. Business Intelligence and Reporting). Be able to differentiate applicability of Redshift vs RDS. Neptune I don‚Äôt recall many questions here. However, you should have general understanding of graph database concepts and graph database applications. I suggest being familiar with its technical architecture Know supported APIs (Gremlin, SPARQL) and models (Property Graph/TinkerPop and W3C RDF/SPARQL) Security Understand options and solutions for data encryption (both data at rest and data in transit) Authentication options and capabilities Access Management (IAM) Have good understanding of services like Parameter Store and KMS Networking Understand how to integrate various services with VPC (e.g. VPC Endpoints) Know how to secure access at network level Solutions Architecture There were a number of questions very reminiscent of questions from AWS Solutions Architect Exam. I strongly suggest attaining AWS Solutions Architect Associate Certification before attempting this specialty exam. Understand AWS Lambda and how it can be used in conjunction with all the other services mentioned above Expect questions on High Availability, Disaster Recovery, and Fault Tolerance Deploy","date":"2020-01-03","objectID":"/aws-certified-database-specialty-exam-guide/:2:0","tags":["AWS","Certification","Database","Study Guide","Tips and Tricks"],"title":"AWS Certified Database - Specialty Exam Preparation Guide and Study Tips","uri":"/aws-certified-database-specialty-exam-guide/"},{"categories":null,"content":"General Tips As mentioned above, I strongly suggest having both AWS Solutions Architect - Associate and AWS Certified Developer - Associate certifications (or preferably AWS DevOps Engineer - Professional) prior to entering to write this specialty exam. The experience and knowledge required to attain those certifications will go a long way in helping you do well in this exam. As suggested by AWS, you should have significant experience designing and implementing solutions consisting of various database technologies. This experience should include considerable understanding and practical use of AWS services. Lastly, I suggest reviewing official AWS documentation, AWS Whitepapers \u0026 Guides, blogs, and videos. In particular, I found advanced level Re:Invent and Online Tech Talk videos particularly useful. ","date":"2020-01-03","objectID":"/aws-certified-database-specialty-exam-guide/:3:0","tags":["AWS","Certification","Database","Study Guide","Tips and Tricks"],"title":"AWS Certified Database - Specialty Exam Preparation Guide and Study Tips","uri":"/aws-certified-database-specialty-exam-guide/"},{"categories":null,"content":"Resources Below is a list of few links I found especially informative. ","date":"2020-01-03","objectID":"/aws-certified-database-specialty-exam-guide/:4:0","tags":["AWS","Certification","Database","Study Guide","Tips and Tricks"],"title":"AWS Certified Database - Specialty Exam Preparation Guide and Study Tips","uri":"/aws-certified-database-specialty-exam-guide/"},{"categories":null,"content":"AWS Re:Invent Videos and Online Tech Talks: Amazon Relational Database Service (Amazon RDS) AWS re:Invent 2017: Deep Dive on Amazon Relational Database Service (RDS) (DAT302) AWS re:Invent 2018: Aurora Serverless: Scalable, Cost-Effective Application Deployment (DAT336) Migrating Microsoft SQL to AWS - AWS Online Tech Talks Contains a good demo of DMS AWS re:Invent 2017: ElastiCache Deep Dive: Best Practices and Usage Patterns (DAT305) AWS re:Invent 2018: ElastiCache Deep Dive: Design Patterns for In-Memory Data Stores (DAT302-R1) AWS re:Invent 2018: Amazon DynamoDB Under the Hood: How We Built a Hyper-Scale Database (DAT321) AWS re:Invent 2018: Accelerate Database Development and Testing with Amazon Aurora (DAT313) Data Design and Modeling for Microservices AWS re:Invent 2017: Best Practices for Data Warehousing with Amazon Redshift \u0026 Redshift Spectrum (ABD304) AWS re:Invent 2018: Amazon DynamoDB Deep Dive: Advanced Design Patterns for DynamoDB (DAT401) ","date":"2020-01-03","objectID":"/aws-certified-database-specialty-exam-guide/:4:1","tags":["AWS","Certification","Database","Study Guide","Tips and Tricks"],"title":"AWS Certified Database - Specialty Exam Preparation Guide and Study Tips","uri":"/aws-certified-database-specialty-exam-guide/"},{"categories":null,"content":"AWS Whitepapers \u0026 Guides An Overview of AWS Cloud Data Migration Services Migrating Applications Running Relational Databases to AWS: Best Practices Guide AWS Database Migration Service Best Practices Migrating Your Databases to Amazon Aurora Best Practices for Migrating MySQL Databases to Amazon Aurora Strategies for Migrating Oracle Databases to AWS Best Practices for Running Oracle Database on AWS Deploying Microsoft SQL Server on AWS Best Practices for Deploying Microsoft SQL Server on AWS Performance at Scale with Amazon ElastiCache Database Caching Strategies Using Redis Getting Started with Amazon DocumentDB (with MongoDB Compatibility) ","date":"2020-01-03","objectID":"/aws-certified-database-specialty-exam-guide/:4:2","tags":["AWS","Certification","Database","Study Guide","Tips and Tricks"],"title":"AWS Certified Database - Specialty Exam Preparation Guide and Study Tips","uri":"/aws-certified-database-specialty-exam-guide/"},{"categories":null,"content":"Final Thoughts In my opinion, AWS Database Specialty is an excellent addition to the AWS Certification program. Database technologies are an integral and crucial part of any cloud based solution. Having a strong competency in these technologies is critical to being a successful architect and designing apt solutions. I hope this guide and contents within are helpful to those preparing to attempt the exam in the future. Good Luck! ","date":"2020-01-03","objectID":"/aws-certified-database-specialty-exam-guide/:5:0","tags":["AWS","Certification","Database","Study Guide","Tips and Tricks"],"title":"AWS Certified Database - Specialty Exam Preparation Guide and Study Tips","uri":"/aws-certified-database-specialty-exam-guide/"},{"categories":null,"content":"I am a Cloud Solutions architect and DevOps Engineer with varied experience in system implementation and integration at large enterprise clients across number of industries (federal public sector, financial services, retail). My expertise focus is in delivery of end-to-end enterprise architectures leveraging public cloud providers Microsoft Azure and Amazon Web Services (AWS) and solution design utilizing cloud native technologies, particularly Kubernetes. Further, I have a strong background in enterprise security and DevOps delivery methods. Throughout my varied experience, I‚Äôve had opportunities to engage with and work alongside clients at all levels of seniority and diverse industry backgrounds, helping them utilize modern technology applications to transform their businesses. With my expertise, I guide customers through various architectural design decisions and tradeoffs, while always keeping client‚Äôs goals and expectations in focus. I have obtained a number of relevant and prominent certifications: Certified Kubernetes Administrator (CKA), Certified Kubernetes Application Developer (CKAD), Azure Solutions Architect Expert, Azure DevOps Engineer Expert, Azure Developer Associate, Azure Security Engineer Associate, AWS Certified Solution Architect (Professional), AWS Certified DevOps Engineer (Professional), AWS Certified Developer, AWS Security Specialty Certified, AWS Database Specialty Certified, HashiCorp Certified: Terraform Associate, TOGAF Certified, and ITIL Certified. In addition, I hold Canada Top Secret Clearance. Besides my professional activities, I volunteer as a technical manuscript reviewer for Manning publications. It‚Äôs something I particularly enjoy because it grants me exposure to new technical topics (and I receive free books!) I am also very active in a number of local community technology groups. Prior to my professional career, I completed a Master of Science in Physics degree from University of Waterloo. I also hold a Bachelor of Science in Computer Science degree and a Bachelor of Science in Physics degree from University of New Brunswick. My full detailed CV is available here ","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"About Me","uri":"/about/"}]